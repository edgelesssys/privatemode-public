apiVersion: apps/v1
kind: Deployment
metadata:
  name: secret-service
  namespace: continuum-bafcc5
  labels:
    app.kubernetes.io/instance: 'bafcc5'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: secret-service
  template:
    metadata:
      labels:
        app: secret-service
        app.kubernetes.io/instance: 'bafcc5'
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: secret-service
          image: 'ghcr.io/edgelesssys/privatemode/secret-service:v1.16.0@sha256:b2452cabed9269bfd773a418bc1a3e3e7534cf37bf8625ef6b31566e54d770f1'
          args:
            - '--etcd-host=secret-service-internal.continuum-bafcc5.svc.cluster.local'
            - '--etcd-server-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-server-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
          ports:
            - containerPort: 3000
            - containerPort: 3001
            - containerPort: 9000
            - containerPort: 2379
            - containerPort: 2380
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: workload-codegen
  namespace: continuum-bafcc5
  labels:
    app: workload-codegen
    app.kubernetes.io/instance: 'bafcc5'
spec:
  replicas: 0
  selector:
    matchLabels:
      app: workload-codegen
  serviceName: workload-codegen
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: workload-codegen
        app.kubernetes.io/instance: 'bafcc5'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        io.katacontainers.config.hypervisor.default_memory: "64000"
        io.katacontainers.config.runtime.create_container_timeout: "1800"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-codegen-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.16.0@sha256:b2eb7219dfedf9e7ed7f77d194df8049b8e4d9dc8f0f869b22b3edaa553f9d28
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              ln -sf /usr/lib64/libnvidia-ml.so.550.90.07 /usr/lib64/libnvidia-ml.so.1 && \
              LD_PRELOAD=/usr/lib64/libcuda.so.1:usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=535.104.05,535.129.03,550.90.07 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.16.0@sha256:03aef9387ba907c7d84106c93fd352409a2ffc6f8b02b6b893e38f90c3062761
          args:
            - --root-hash=c6ab549d3ec1cb75d531886229825deec19e43e1dbc35480fd82a0517f04c14c
            - --device-path=/dev/modelweights
            - --mount-path=/mnt/modelweights
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights
              devicePath: /dev/modelweights
      containers:
        - name: workload
          image: docker.io/vllm/vllm-openai:v0.8.5.post1@sha256:c48cf118e1e6e39d7790e174d6014f7af5d06f79c2d29d984d11cbe2e8d414e7
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--no-enable-prefix-caching'
            - '--host=0.0.0.0'
            - '--model=/mnt/modelweights'
            - '--served-model-name'
            - 'mistralai/Codestral-22B-v0.1'
            - '--disable-log-requests'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.16.0@sha256:5491f426a6a13fac8c858d4b065d0f68c224c37b22c2c1383d5e41b93d530dab
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=openai
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-bafcc5.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-task=generate'
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-codegen
      volumes:
        - emptyDir: {}
          name: mount-dir
  volumeClaimTemplates:
    - kind: PersistentVolumeClaim
      metadata:
        name: modelweights
        labels:
          app.kubernetes.io/instance: 'bafcc5'
      spec:
        storageClassName: mistralai-codestral-22b-v0-1
        volumeMode: Block
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 46Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: workload-embeddings
  namespace: continuum-bafcc5
  labels:
    app: workload-embeddings
    app.kubernetes.io/instance: 'bafcc5'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-embeddings
  serviceName: workload-embeddings
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: workload-embeddings
        app.kubernetes.io/instance: 'bafcc5'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        io.katacontainers.config.hypervisor.default_memory: "64000"
        io.katacontainers.config.runtime.create_container_timeout: "1800"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-embeddings-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.16.0@sha256:b2eb7219dfedf9e7ed7f77d194df8049b8e4d9dc8f0f869b22b3edaa553f9d28
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              ln -sf /usr/lib64/libnvidia-ml.so.550.90.07 /usr/lib64/libnvidia-ml.so.1 && \
              LD_PRELOAD=/usr/lib64/libcuda.so.1:usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=535.104.05,535.129.03,550.90.07 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.16.0@sha256:03aef9387ba907c7d84106c93fd352409a2ffc6f8b02b6b893e38f90c3062761
          args:
            - --root-hash=98fcf1a13ce17114d7647073feebaff00d709f9804d31be00e918d2ea4fa0df4
            - --device-path=/dev/modelweights
            - --mount-path=/mnt/modelweights
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights
              devicePath: /dev/modelweights
      containers:
        - name: workload
          image: docker.io/vllm/vllm-openai:v0.8.5.post1@sha256:c48cf118e1e6e39d7790e174d6014f7af5d06f79c2d29d984d11cbe2e8d414e7
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--no-enable-prefix-caching'
            - '--host=0.0.0.0'
            - '--model=/mnt/modelweights'
            - '--served-model-name'
            - 'intfloat/multilingual-e5-large-instruct'
            - '--disable-log-requests'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.16.0@sha256:5491f426a6a13fac8c858d4b065d0f68c224c37b22c2c1383d5e41b93d530dab
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=openai
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-bafcc5.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-task=embed'
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-embeddings
      volumes:
        - emptyDir: {}
          name: mount-dir
  volumeClaimTemplates:
    - kind: PersistentVolumeClaim
      metadata:
        name: modelweights
        labels:
          app.kubernetes.io/instance: 'bafcc5'
      spec:
        storageClassName: intfloat-multilingual-e5-large-instruct
        volumeMode: Block
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: workload-multimodal
  namespace: continuum-bafcc5
  labels:
    app: workload-multimodal
    app.kubernetes.io/instance: 'bafcc5'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-multimodal
  serviceName: workload-multimodal
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: workload-multimodal
        app.kubernetes.io/instance: 'bafcc5'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        io.katacontainers.config.hypervisor.default_memory: "64000"
        io.katacontainers.config.runtime.create_container_timeout: "1800"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-multimodal-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.16.0@sha256:b2eb7219dfedf9e7ed7f77d194df8049b8e4d9dc8f0f869b22b3edaa553f9d28
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              ln -sf /usr/lib64/libnvidia-ml.so.550.90.07 /usr/lib64/libnvidia-ml.so.1 && \
              LD_PRELOAD=/usr/lib64/libcuda.so.1:usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=535.104.05,535.129.03,550.90.07 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.16.0@sha256:03aef9387ba907c7d84106c93fd352409a2ffc6f8b02b6b893e38f90c3062761
          args:
            - --root-hash=cc7c7c8381b5bb62fe6b5a34bb95eb81b44463658e9a3f57fb2700edd798b737
            - --device-path=/dev/modelweights
            - --mount-path=/mnt/modelweights
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights
              devicePath: /dev/modelweights
      containers:
        - name: workload
          image: docker.io/vllm/vllm-openai:v0.8.4@sha256:b168cbb0101f51b2491047345d0a83f5a8ecbe56a6604f2a2edb81eca55ebc9e
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--no-enable-prefix-caching'
            - '--host=0.0.0.0'
            - '--model=/mnt/modelweights'
            - '--served-model-name'
            - 'google/gemma-3-27b-it'
            - '--disable-log-requests'
            - '--max-model-len=20000'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.16.0@sha256:5491f426a6a13fac8c858d4b065d0f68c224c37b22c2c1383d5e41b93d530dab
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=openai
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-bafcc5.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-task=generate'
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-multimodal
      volumes:
        - emptyDir: {}
          name: mount-dir
  volumeClaimTemplates:
    - kind: PersistentVolumeClaim
      metadata:
        name: modelweights
        labels:
          app.kubernetes.io/instance: 'bafcc5'
      spec:
        storageClassName: google-gemma-3-27b-it
        volumeMode: Block
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 75Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: workload-textgen
  namespace: continuum-bafcc5
  labels:
    app: workload-textgen
    app.kubernetes.io/instance: 'bafcc5'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-textgen
  serviceName: workload-textgen
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: workload-textgen
        app.kubernetes.io/instance: 'bafcc5'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        io.katacontainers.config.hypervisor.default_memory: "64000"
        io.katacontainers.config.runtime.create_container_timeout: "1800"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-textgen-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.16.0@sha256:b2eb7219dfedf9e7ed7f77d194df8049b8e4d9dc8f0f869b22b3edaa553f9d28
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              ln -sf /usr/lib64/libnvidia-ml.so.550.90.07 /usr/lib64/libnvidia-ml.so.1 && \
              LD_PRELOAD=/usr/lib64/libcuda.so.1:usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=535.104.05,535.129.03,550.90.07 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.16.0@sha256:03aef9387ba907c7d84106c93fd352409a2ffc6f8b02b6b893e38f90c3062761
          args:
            - --root-hash=3a49ed30e4a637228b5d962c6b1fa508bad337ca73ef02cb9f0dcc7931fefad7
            - --device-path=/dev/modelweights
            - --mount-path=/mnt/modelweights
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights
              devicePath: /dev/modelweights
      containers:
        - name: workload
          image: docker.io/vllm/vllm-openai:v0.8.5.post1@sha256:c48cf118e1e6e39d7790e174d6014f7af5d06f79c2d29d984d11cbe2e8d414e7
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--no-enable-prefix-caching'
            - '--host=0.0.0.0'
            - '--model=/mnt/modelweights'
            - '--served-model-name'
            - 'ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4'
            - 'latest'
            - '--disable-log-requests'
            - '--max-model-len=70000'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'llama3_json'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.16.0@sha256:5491f426a6a13fac8c858d4b065d0f68c224c37b22c2c1383d5e41b93d530dab
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=openai
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-bafcc5.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-task=generate'
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-textgen
      volumes:
        - emptyDir: {}
          name: mount-dir
  volumeClaimTemplates:
    - kind: PersistentVolumeClaim
      metadata:
        name: modelweights
        labels:
          app.kubernetes.io/instance: 'bafcc5'
      spec:
        storageClassName: ibnzterrell-meta-llama-3-3-70b-instruct-awq-int4
        volumeMode: Block
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 41Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unstructured-api
  namespace: continuum-bafcc5
  labels:
    app: unstructured-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: unstructured-api
  template:
    metadata:
      labels:
        app: unstructured-api
        app.kubernetes.io/instance: 'bafcc5'
      annotations:
        io.katacontainers.config.runtime.create_container_timeout: "600"
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: unstructured-api
          image: 'quay.io/unstructured-io/unstructured-api:0.0.84@sha256:b343bd8c1c665ee2e0cfbf2fb55d36d8344c4e37b719135af954fbca736fb61e'
          resources:
            requests:
              memory: "24Gi"
            limits:
              memory: "24Gi"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.16.0@sha256:5491f426a6a13fac8c858d4b065d0f68c224c37b22c2c1383d5e41b93d530dab
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=unstructured
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-bafcc5.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coordinator
  namespace: 'continuum-bafcc5'
  labels:
    app.kubernetes.io/instance: 'bafcc5'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: coordinator
  namespace: 'continuum-bafcc5'
  labels:
    app.kubernetes.io/instance: 'bafcc5'
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - create
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: coordinator
  namespace: 'continuum-bafcc5'
  labels:
    app.kubernetes.io/instance: 'bafcc5'
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: coordinator
subjects:
  - kind: ServiceAccount
    name: coordinator
    namespace: "continuum-bafcc5"
---
apiVersion: v1
kind: Service
metadata:
  name: coordinator
  namespace: 'continuum-bafcc5'
  labels:
    app.kubernetes.io/instance: 'bafcc5'
spec:
  ports:
    - name: userapi
      port: 1313
      targetPort: 1313
    - name: meshapi
      port: 7777
      targetPort: 7777
    - name: default-userapi
      port: 443
      targetPort: 1313
  selector:
    app.kubernetes.io/name: coordinator
    app.kubernetes.io/instance: 'bafcc5'
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: coordinator
  namespace: 'continuum-bafcc5'
  labels:
    app.kubernetes.io/instance: 'bafcc5'
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: coordinator
  serviceName: coordinator
  template:
    metadata:
      annotations:
        contrast.edgeless.systems/pod-role: coordinator
      labels:
        app.kubernetes.io/name: coordinator
        app.kubernetes.io/instance: 'bafcc5'
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    contrast.edgeless.systems/pod-role: coordinator
                topologyKey: kubernetes.io/hostname
              weight: 100
      containers:
        - image: ghcr.io/edgelesssys/contrast/coordinator:v1.7.0@sha256:94eed95eca7f497ae516c18343aca5a604142bd7a92318c41c7e0cd5e5b20bcb
          name: coordinator
          ports:
            - containerPort: 1313
              name: userapi
            - containerPort: 7777
              name: meshapi
          readinessProbe:
            initialDelaySeconds: 1
            periodSeconds: 5
            tcpSocket:
              port: 1313
          resources:
            limits:
              memory: 200Mi
            requests:
              memory: 200Mi
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
          volumeDevices:
            - devicePath: /dev/csi0
              name: state-device
      runtimeClassName: contrast-cc
      serviceAccountName: coordinator
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: state-device
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        volumeMode: Block
