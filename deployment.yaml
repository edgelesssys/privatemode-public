apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: secret-service
  namespace: continuum-33df75
  labels:
    app.kubernetes.io/instance: '33df75'
spec:
  replicas: 3
  serviceName: secret-service-headless
  selector:
    matchLabels:
      app: secret-service
  template:
    metadata:
      labels:
        app: secret-service
        app.kubernetes.io/instance: '33df75'
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: secret-service
          image: 'ghcr.io/edgelesssys/privatemode/secret-service:v1.21.0@sha256:6c6c4cb1c2ad8f0d627db1c8f5e885ff5161a1c752093255770dca48d029b526'
          command: ["/bin/sh", "-c"]
          args:
            - |
              exec /bin/secret-service \
                --k8s-namespace=continuum-33df75 \
                $([ "${HOSTNAME##*-}" = "0" ] && echo "--may-bootstrap") \
                --etcd-server-cert=/contrast/tls-config/certChain.pem \
                --etcd-server-key=/contrast/tls-config/key.pem \
                --etcd-ca=/contrast/tls-config/mesh-ca.pem
          ports:
            - containerPort: 3000
            - containerPort: 3001
            - containerPort: 9000
            - containerPort: 2379
            - containerPort: 2380
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-embeddings-transcriptions
  namespace: continuum-33df75
  labels:
    app: workload-embeddings-transcriptions
    app.kubernetes.io/instance: '33df75'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-embeddings-transcriptions
  template:
    metadata:
      labels:
        app: workload-embeddings-transcriptions
        app.kubernetes.io/instance: '33df75'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-embeddings-transcriptions-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.21.0@sha256:5c1a77733d7d57178ba38167425590ef8f34af93b3debee49e686a6b467f80d7
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              LD_PRELOAD=/usr/lib64/libcuda.so.1:/usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=570.158.01 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-embeddings-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.21.0@sha256:b8a9230b57d7ec06766f119681b8eda7df20f45c45c18618e92524bb3e5e937c
          args:
            - --root-hash=a9f64605a1101d089fd9f380313da39ff8abeb5e22df74d6ab27c33eeb89153e
            - --device-path=/dev/modelweights-embeddings-0
            - --mount-path=/mnt/modelweights-embeddings-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-embeddings-0
              devicePath: /dev/modelweights-embeddings-0
        - name: disk-mounter-transcriptions-1
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.21.0@sha256:b8a9230b57d7ec06766f119681b8eda7df20f45c45c18618e92524bb3e5e937c
          args:
            - --root-hash=c90475439e9a59606b462320a906c7f23b341c4a2dc92f3a15f964aed9b4b545
            - --device-path=/dev/modelweights-transcriptions-1
            - --mount-path=/mnt/modelweights-transcriptions-1
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-transcriptions-1
              devicePath: /dev/modelweights-transcriptions-1
      containers:
        - name: workload-embeddings-0
          image: docker.io/vllm/vllm-openai:v0.9.2@sha256:37cd5bd18d220a0f4c70401ce1d4a0cc588fbfe03cc210579428f2c47e6eac33
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-embeddings-0'
            - '--served-model-name'
            - 'intfloat/multilingual-e5-large-instruct'
            - '--disable-log-requests'
            - '--prefix-caching-hash-algo=sha256'
            - '--enable-prompt-tokens-details'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-embeddings-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.21.0@sha256:8c654ac4da2e6b00b6b74cbb245e3c3cfbd99cb05fd778c500e3fc8afa892bdd
          ports:
            - containerPort: 8085
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-33df75.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=embed'
          volumeMounts:
            - name: inference-proxy-embeddings-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
        - name: workload-transcriptions-1
          image: docker.io/vllm/vllm-openai:v0.9.2@sha256:37cd5bd18d220a0f4c70401ce1d4a0cc588fbfe03cc210579428f2c47e6eac33
          ports:
            - containerPort: 8001
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          command:
            - /bin/sh
            - -ec
            - |
              if [ $(sha256sum /vllm-workspace/pylock/pylock.toml | cut -d ' ' -f 1) != '51c224d7c47fd486eddfad28de009fefacc547537935bb2da201b4e6747155d3' ]; then echo pylock.toml hash mismatch: $(sha256sum /vllm-workspace/pylock/pylock.toml | cut -d ' ' -f 1); exit 1; fi
              uv pip install -r /vllm-workspace/pylock/pylock.toml --system
              exec python3 -m vllm.entrypoints.openai.api_server "$@"
            - _
          args:
            - '--host=0.0.0.0'
            - '--port=8001'
            - '--model=/mnt/modelweights-transcriptions-1'
            - '--served-model-name'
            - 'openai/whisper-large-v3'
            - '--disable-log-requests'
            - '--prefix-caching-hash-algo=sha256'
            - '--enable-prompt-tokens-details'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-audio-pylock
              mountPath: /vllm-workspace/pylock
              readOnly: true
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
        - name: inference-proxy-transcriptions-1
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.21.0@sha256:8c654ac4da2e6b00b6b74cbb245e3c3cfbd99cb05fd778c500e3fc8afa892bdd
          ports:
            - containerPort: 8086
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8086'
            - '--workload-port=8001'
            - '--secret-svc-address=secret-service-internal.continuum-33df75.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=transcribe,translate'
          volumeMounts:
            - name: inference-proxy-transcriptions-1-tls-cert
              mountPath: /etc/tls
              readOnly: true
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-embeddings-transcriptions
      volumes:
        - emptyDir: {}
          name: mount-dir
        - name: inference-proxy-embeddings-0-tls-cert
          secret:
            secretName: inference-proxy-embeddings-0-tls-secret
        - name: vllm-audio-pylock
          configMap:
            name: vllm-audio-pylock
        - name: inference-proxy-transcriptions-1-tls-cert
          secret:
            secretName: inference-proxy-transcriptions-1-tls-secret
        - name: modelweights-embeddings-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: intfloat-multilingual-e5-large-instruct-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 5Gi
        - name: modelweights-transcriptions-1
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: openai-whisper-large-v3-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 26Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-multimodal
  namespace: continuum-33df75
  labels:
    app: workload-multimodal
    app.kubernetes.io/instance: '33df75'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-multimodal
  template:
    metadata:
      labels:
        app: workload-multimodal
        app.kubernetes.io/instance: '33df75'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-multimodal-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.21.0@sha256:5c1a77733d7d57178ba38167425590ef8f34af93b3debee49e686a6b467f80d7
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              LD_PRELOAD=/usr/lib64/libcuda.so.1:/usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=570.158.01 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-multimodal-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.21.0@sha256:b8a9230b57d7ec06766f119681b8eda7df20f45c45c18618e92524bb3e5e937c
          args:
            - --root-hash=aea72e2345a0ba4ab7f8a27c86cfbe0d98b8fa4d2fc449bc29103e928b4143b3
            - --device-path=/dev/modelweights-multimodal-0
            - --mount-path=/mnt/modelweights-multimodal-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-multimodal-0
              devicePath: /dev/modelweights-multimodal-0
      containers:
        - name: workload-multimodal-0
          image: docker.io/vllm/vllm-openai:v0.9.0.1@sha256:6e128f5e60fcb8b8ca76eb63f102c0d96c34a7ef4ff014df920eb3eb70dd9193
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-multimodal-0'
            - --mm-processor-kwargs
            - '{"do_pan_and_scan": true}'
            - '--served-model-name'
            - 'leon-se/gemma-3-27b-it-fp8-dynamic'
            - '--disable-log-requests'
            - '--max-model-len=70000'
            - '--prefix-caching-hash-algo=sha256'
            - '--enable-prompt-tokens-details'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-multimodal-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.21.0@sha256:8c654ac4da2e6b00b6b74cbb245e3c3cfbd99cb05fd778c500e3fc8afa892bdd
          ports:
            - containerPort: 8085
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-33df75.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=generate,vision'
          volumeMounts:
            - name: inference-proxy-multimodal-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-multimodal
      volumes:
        - emptyDir: {}
          name: mount-dir
        - name: inference-proxy-multimodal-0-tls-cert
          secret:
            secretName: inference-proxy-multimodal-0-tls-secret
        - name: modelweights-multimodal-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: leon-se-gemma-3-27b-it-fp8-dynamic-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 30Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-textgen
  namespace: continuum-33df75
  labels:
    app: workload-textgen
    app.kubernetes.io/instance: '33df75'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-textgen
  template:
    metadata:
      labels:
        app: workload-textgen
        app.kubernetes.io/instance: '33df75'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8000"
    spec:
      serviceAccountName: workload-textgen-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.21.0@sha256:5c1a77733d7d57178ba38167425590ef8f34af93b3debee49e686a6b467f80d7
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - '/bin/sh'
            - '-c'
          args:
            - |
              LD_PRELOAD=/usr/lib64/libcuda.so.1:/usr/lib64/libnvidia-ml.so.1 attestation-agent \
                --gpu-debug=false \
                --gpu-secure-boot=true \
                --gpu-eat-version=EAT-21 \
                --gpu-driver-versions=570.158.01 \
                --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-textgen-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.21.0@sha256:b8a9230b57d7ec06766f119681b8eda7df20f45c45c18618e92524bb3e5e937c
          args:
            - --root-hash=75b8d152c9e012fe24ab50a60346a8f874aa35091e1ead2045bf326a3aa0f67e
            - --device-path=/dev/modelweights-textgen-0
            - --mount-path=/mnt/modelweights-textgen-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-textgen-0
              devicePath: /dev/modelweights-textgen-0
      containers:
        - name: workload-textgen-0
          image: docker.io/vllm/vllm-openai:v0.9.2@sha256:37cd5bd18d220a0f4c70401ce1d4a0cc588fbfe03cc210579428f2c47e6eac33
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LD_PRELOAD
              value: "/usr/lib/x86_64-linux-gnu/libcuda.so.1"
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-textgen-0'
            - '--served-model-name'
            - 'ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4'
            - 'latest'
            - '--disable-log-requests'
            - '--max-model-len=70000'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'llama3_json'
            - '--prefix-caching-hash-algo=sha256'
            - '--enable-prompt-tokens-details'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-textgen-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.21.0@sha256:8c654ac4da2e6b00b6b74cbb245e3c3cfbd99cb05fd778c500e3fc8afa892bdd
          ports:
            - containerPort: 8085
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-33df75.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=generate,tool_calling'
          volumeMounts:
            - name: inference-proxy-textgen-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-textgen
      volumes:
        - emptyDir: {}
          name: mount-dir
        - name: inference-proxy-textgen-0-tls-cert
          secret:
            secretName: inference-proxy-textgen-0-tls-secret
        - name: modelweights-textgen-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: ibnzterrell-meta-llama-3-3-70b-instruct-awq-int4-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 41Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unstructured-api
  namespace: continuum-33df75
  labels:
    app: unstructured-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: unstructured-api
  template:
    metadata:
      labels:
        app: unstructured-api
        app.kubernetes.io/instance: '33df75'
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: unstructured-api
          image: 'quay.io/unstructured-io/unstructured-api:0.0.89@sha256:c00345fb0c58c3147136ec2edbeee11ec7849d8b6a402726b6568dbff0a8b163'
          command: ["uvicorn"]
          args:
            - prepline_general.api.app:app
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"
            - "--log-level"
            - "info"
          resources:
            requests:
              memory: "24Gi"
            limits:
              memory: "24Gi"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.21.0@sha256:8c654ac4da2e6b00b6b74cbb245e3c3cfbd99cb05fd778c500e3fc8afa892bdd
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=unstructured
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-33df75.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
          volumeMounts:
            - name: unstructured-api-tls-cert
              mountPath: /etc/tls
              readOnly: true
      volumes:
        - name: unstructured-api-tls-cert
          secret:
            secretName: unstructured-api-tls-secret
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coordinator
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: coordinator
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - create
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: coordinator
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: coordinator
subjects:
  - kind: ServiceAccount
    name: coordinator
    namespace: "continuum-33df75"
---
apiVersion: v1
kind: Service
metadata:
  name: coordinator
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
spec:
  ports:
    - name: userapi
      port: 1313
      targetPort: 1313
    - name: meshapi
      port: 7777
      targetPort: 7777
    - name: transitapi
      port: 8200
      targetPort: 8200
    - name: default-userapi
      port: 443
      targetPort: 1313
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: coordinator
    app.kubernetes.io/instance: '33df75'
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: coordinator-ready
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
spec:
  ports:
    - name: userapi
      port: 1313
      targetPort: 1313
    - name: meshapi
      port: 7777
      targetPort: 7777
    - name: transitapi
      port: 8200
      targetPort: 8200
    - name: default-userapi
      port: 443
      targetPort: 1313
  selector:
    app.kubernetes.io/name: coordinator
    app.kubernetes.io/instance: '33df75'
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: coordinator
  namespace: 'continuum-33df75'
  labels:
    app.kubernetes.io/instance: '33df75'
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: coordinator
  serviceName: coordinator
  template:
    metadata:
      annotations:
        contrast.edgeless.systems/pod-role: coordinator
      labels:
        app.kubernetes.io/name: coordinator
        app.kubernetes.io/instance: '33df75'
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    contrast.edgeless.systems/pod-role: coordinator
                topologyKey: kubernetes.io/hostname
              weight: 100
      containers:
        - image: ghcr.io/edgelesssys/contrast/coordinator:v1.10.0@sha256:a5572d6fab1d140398ce8e2247aa5aa9d12445bf4d95d02d8e6b0d8e207039c8
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /probe/liveness
              port: 9102
            periodSeconds: 10
          name: coordinator
          ports:
            - containerPort: 1313
              name: userapi
            - containerPort: 7777
              name: meshapi
            - containerPort: 8200
              name: transitapi
          readinessProbe:
            httpGet:
              path: /probe/readiness
              port: 9102
            periodSeconds: 5
          resources:
            limits:
              memory: 200Mi
            requests:
              memory: 200Mi
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /probe/startup
              port: 9102
            initialDelaySeconds: 1
            periodSeconds: 1
      runtimeClassName: contrast-cc
      serviceAccountName: coordinator
