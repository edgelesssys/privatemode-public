apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: secret-service
  namespace: continuum-c8cc26
  labels:
    app.kubernetes.io/instance: 'c8cc26'
spec:
  replicas: 3
  serviceName: secret-service-headless
  selector:
    matchLabels:
      app: secret-service
  template:
    metadata:
      labels:
        app: secret-service
        app.kubernetes.io/instance: 'c8cc26'
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: secret-service
          image: 'ghcr.io/edgelesssys/privatemode/secret-service:v1.34.0@sha256:07c5fbc5b599150f4f2143ffd02de4a00720cb46c7d127b346a487b3a456e6e4'
          command: ["/bin/sh", "-c"]
          args:
            - |
              exec /bin/secret-service \
                --k8s-namespace=continuum-c8cc26 \
                $([ "${HOSTNAME##*-}" = "0" ] && echo "--may-bootstrap") \
                --etcd-server-cert=/contrast/tls-config/certChain.pem \
                --etcd-server-key=/contrast/tls-config/key.pem \
                --etcd-ca=/contrast/tls-config/mesh-ca.pem
          ports:
            - containerPort: 3000
            - containerPort: 3001
            - containerPort: 9000
            - containerPort: 2379
            - containerPort: 2380
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-gptoss
  namespace: continuum-c8cc26
  labels:
    app: workload-gptoss
    app.kubernetes.io/instance: 'c8cc26'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-gptoss
  template:
    metadata:
      labels:
        app: workload-gptoss
        app.kubernetes.io/instance: 'c8cc26'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/scheme: "http"
    spec:
      serviceAccountName: workload-gptoss-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          volumeMounts:
            - name: privatemode-ca-cert
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
            - mountPath: /contrast
              name: contrast-secrets
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.34.0@sha256:735407de699cfb96a51e78e59252e813e46390d8adf992b5ff062fa1c3e4516d
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - --gpu-driver-versions=590.48.01
            - --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-gptoss-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-gptoss-0
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=682ea81fb02151c70b8c281159559b14b9bc4b0368ad4fbb50203f556ad3ddc3
            - --device-path=/dev/modelweights-gptoss-0
            - --mount-path=/mnt/modelweights-gptoss-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-gptoss-0
              devicePath: /dev/modelweights-gptoss-0
      containers:
        - name: workload-gptoss-0
          image: docker.io/vllm/vllm-openai:v0.11.2@sha256:47a9896f86818fea323b2d38082758c62d9a0155d6fe6c4dbd7d735c556f680a
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - /bin/sh
            - -ec
            - |
              cp /untrusted-vllm-patch/gpt-oss-tools.patch /vllm-workspace/gpt-oss-tools.patch
              if [ $(sha256sum /vllm-workspace/gpt-oss-tools.patch | cut -d ' ' -f 1) != 'db86a3f86dad392c485add6aa8fcb6b70d233a89d104c2a4fbb4d02bb6a1b678' ]; then echo gpt-oss-tools.patch hash mismatch: $(sha256sum /vllm-workspace/gpt-oss-tools.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 /usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py < /vllm-workspace/gpt-oss-tools.patch
              cp /untrusted-vllm-patch/openai-tool-parser.patch /vllm-workspace/openai-tool-parser.patch
              if [ $(sha256sum /vllm-workspace/openai-tool-parser.patch | cut -d ' ' -f 1) != '8dc4e52544b9f3fd8adbd366be908eaa7fce9899e96c417c40c56a6eba6190f4' ]; then echo openai-tool-parser.patch hash mismatch: $(sha256sum /vllm-workspace/openai-tool-parser.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 /usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py < /vllm-workspace/openai-tool-parser.patch
              cp /untrusted-vllm-patch/anon-log.patch /vllm-workspace/anon-log.patch
              if [ $(sha256sum /vllm-workspace/anon-log.patch | cut -d ' ' -f 1) != 'd3fa9c7063e7b41d4c4748782b864dd1e9463d9300bc7f58cc3a4bc3b15b07fe' ]; then echo anon-log.patch hash mismatch: $(sha256sum /vllm-workspace/anon-log.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 /usr/local/lib/python3.12/dist-packages/vllm/v1/core/sched/output.py < /vllm-workspace/anon-log.patch
              exec python3 -m vllm.entrypoints.openai.api_server "$@"
            - _
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-gptoss-0'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=5.2g'
            - '--max-num-seqs=256'
            - '--max-model-len=131072'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'openai'
            - '--reasoning-parser'
            - 'openai_gptoss'
            - '--structured-outputs-config={"backend": "xgrammar", "disable_any_whitespace": true}'
            - '--prefix-caching-hash-algo=sha256'
            - '--served-model-name'
            - 'openai/gpt-oss-120b'
            - 'gpt-oss-120b'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-gptoss-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8085
            - containerPort: 8185
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--metrics-port=8185'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=generate,tool_calling'
          volumeMounts:
            - name: inference-proxy-gptoss-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-gptoss
      volumes:
        - name: privatemode-ca-cert
          secret:
            secretName: privatemode-ca-secret
            items:
              - key: ca.crt
                path: ca.crt
        - emptyDir: {}
          name: mount-dir
        - emptyDir: {}
          name: continuum-run
        - name: vllm-patch
          configMap:
            name: vllm-patch
        - name: inference-proxy-gptoss-0-tls-cert
          secret:
            secretName: inference-proxy-gptoss-0-tls-secret
        - name: modelweights-gptoss-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: openai-gpt-oss-120b-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 67Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-multimodal-qwenembed
  namespace: continuum-c8cc26
  labels:
    app: workload-multimodal-qwenembed
    app.kubernetes.io/instance: 'c8cc26'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-multimodal-qwenembed
  template:
    metadata:
      labels:
        app: workload-multimodal-qwenembed
        app.kubernetes.io/instance: 'c8cc26'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/scheme: "http"
    spec:
      serviceAccountName: workload-multimodal-qwenembed-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          volumeMounts:
            - name: privatemode-ca-cert
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
            - mountPath: /contrast
              name: contrast-secrets
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.34.0@sha256:735407de699cfb96a51e78e59252e813e46390d8adf992b5ff062fa1c3e4516d
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - --gpu-driver-versions=590.48.01
            - --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-multimodal-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-multimodal-0
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=d7257b7f4d1b0d610fdbe7504f9d2a86f9c3c219917f2b77bb3d05f0b25f317a
            - --device-path=/dev/modelweights-multimodal-0
            - --mount-path=/mnt/modelweights-multimodal-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-multimodal-0
              devicePath: /dev/modelweights-multimodal-0
        - name: disk-mounter-qwenembed-1
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-qwenembed-1
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=a0bd28202038517c620a138d35309c53e7fad50ec634e85a27060906e65e628f
            - --device-path=/dev/modelweights-qwenembed-1
            - --mount-path=/mnt/modelweights-qwenembed-1
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-qwenembed-1
              devicePath: /dev/modelweights-qwenembed-1
      containers:
        - name: workload-multimodal-0
          image: docker.io/vllm/vllm-openai:v0.14.1@sha256:6bf34e50e2387dc46dc87a9d6a945fdd616a022bccfddd949052f54063ebcb8c
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-multimodal-0'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=36.4g'
            - '--max-num-seqs=256'
            - --mm-processor-kwargs
            - '{"do_pan_and_scan": true}'
            - '--max-model-len=131072'
            - '--chat-template'
            - 'examples/tool_chat_template_gemma3_pythonic.jinja'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'pythonic'
            - '--structured-outputs-config={"backend": "xgrammar", "disable_any_whitespace": true}'
            - '--prefix-caching-hash-algo=sha256'
            - '--served-model-name'
            - 'leon-se/gemma-3-27b-it-fp8-dynamic'
            - 'gemma-3-27b'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-multimodal-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8085
            - containerPort: 8185
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--metrics-port=8185'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=generate,tool_calling,vision'
          volumeMounts:
            - name: inference-proxy-multimodal-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
        - name: workload-qwenembed-1
          image: docker.io/vllm/vllm-openai:v0.14.1@sha256:6bf34e50e2387dc46dc87a9d6a945fdd616a022bccfddd949052f54063ebcb8c
          ports:
            - containerPort: 8001
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - '--host=0.0.0.0'
            - '--port=8001'
            - '--model=/mnt/modelweights-qwenembed-1'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=6.4g'
            - '--max-num-seqs=256'
            - '--runner=pooling'
            - --hf-overrides
            - '{"matryoshka_dimensions":[1024,2560]}'
            - '--no-enable-prefix-caching'
            - '--served-model-name'
            - 'qwen3-embedding-4b'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
        - name: inference-proxy-qwenembed-1
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8086
            - containerPort: 8186
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8086'
            - '--metrics-port=8186'
            - '--workload-port=8001'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=embed'
          volumeMounts:
            - name: inference-proxy-qwenembed-1-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-multimodal-qwenembed
      volumes:
        - name: privatemode-ca-cert
          secret:
            secretName: privatemode-ca-secret
            items:
              - key: ca.crt
                path: ca.crt
        - emptyDir: {}
          name: mount-dir
        - emptyDir: {}
          name: continuum-run
        - name: vllm-patch
          configMap:
            name: vllm-patch
        - name: inference-proxy-multimodal-0-tls-cert
          secret:
            secretName: inference-proxy-multimodal-0-tls-secret
        - name: inference-proxy-qwenembed-1-tls-cert
          secret:
            secretName: inference-proxy-qwenembed-1-tls-secret
        - name: modelweights-multimodal-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: leon-se-gemma-3-27b-it-fp8-dynamic-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 30Gi
        - name: modelweights-qwenembed-1
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: boboliu-qwen3-embedding-4b-w4a16-g128
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 3Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-transcriptions-codegen
  namespace: continuum-c8cc26
  labels:
    app: workload-transcriptions-codegen
    app.kubernetes.io/instance: 'c8cc26'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-transcriptions-codegen
  template:
    metadata:
      labels:
        app: workload-transcriptions-codegen
        app.kubernetes.io/instance: 'c8cc26'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/scheme: "http"
    spec:
      serviceAccountName: workload-transcriptions-codegen-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          volumeMounts:
            - name: privatemode-ca-cert
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
            - mountPath: /contrast
              name: contrast-secrets
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.34.0@sha256:735407de699cfb96a51e78e59252e813e46390d8adf992b5ff062fa1c3e4516d
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - --gpu-driver-versions=590.48.01
            - --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-transcriptions-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-transcriptions-0
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=d77fcd381dee672e89843bf782f571ea296833966beaf235f0c45f765473b782
            - --device-path=/dev/modelweights-transcriptions-0
            - --mount-path=/mnt/modelweights-transcriptions-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-transcriptions-0
              devicePath: /dev/modelweights-transcriptions-0
        - name: disk-mounter-codegen-1
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-codegen-1
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=0605976b547cc008c4d202cbd97129715dc47ce526810c5334cf5620694d9d14
            - --device-path=/dev/modelweights-codegen-1
            - --mount-path=/mnt/modelweights-codegen-1
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-codegen-1
              devicePath: /dev/modelweights-codegen-1
      containers:
        - name: workload-transcriptions-0
          image: docker.io/vllm/vllm-openai:v0.14.1@sha256:6bf34e50e2387dc46dc87a9d6a945fdd616a022bccfddd949052f54063ebcb8c
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - /bin/sh
            - -ec
            - |
              cp /untrusted-pylock/pylock.toml /vllm-workspace/pylock.toml
              if [ $(sha256sum /vllm-workspace/pylock.toml | cut -d ' ' -f 1) != '4138144e81ab67708175d57b2fe717e0b8bdbe2bb6527b04eb65ef6aa94a0dcf' ]; then echo pylock.toml hash mismatch: $(sha256sum /vllm-workspace/pylock.toml | cut -d ' ' -f 1); exit 1; fi
              uv pip install -r /vllm-workspace/pylock.toml --system
              cp /untrusted-vllm-patch/transcriptions.patch /vllm-workspace/transcriptions.patch
              if [ $(sha256sum /vllm-workspace/transcriptions.patch | cut -d ' ' -f 1) != '9e006c923b049875f84acad59a74df9946eb721518ef0b3a909b0f52fd3119ff' ]; then echo transcriptions.patch hash mismatch: $(sha256sum /vllm-workspace/transcriptions.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 -p1 -d /usr/local/lib/python3.12/dist-packages < /vllm-workspace/transcriptions.patch
              exec python3 -m vllm.entrypoints.openai.api_server "$@"
            - _
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-transcriptions-0'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=26.0g'
            - '--max-num-seqs=18'
            - '--no-enable-prefix-caching'
            - '--served-model-name'
            - 'openai/whisper-large-v3'
            - 'whisper-large-v3'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-audio-pylock
              mountPath: /untrusted-pylock
              readOnly: true
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 70Gi
            limits:
              memory: 70Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-transcriptions-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8085
            - containerPort: 8185
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--metrics-port=8185'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=transcribe'
          volumeMounts:
            - name: inference-proxy-transcriptions-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
        - name: workload-codegen-1
          image: docker.io/vllm/vllm-openai:v0.14.1@sha256:6bf34e50e2387dc46dc87a9d6a945fdd616a022bccfddd949052f54063ebcb8c
          ports:
            - containerPort: 8001
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8001
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - '--host=0.0.0.0'
            - '--port=8001'
            - '--model=/mnt/modelweights-codegen-1'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=28.0g'
            - '--max-num-seqs=256'
            - '--max-model-len=131072'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'qwen3_coder'
            - '--structured-outputs-config={"backend": "xgrammar", "disable_any_whitespace": true}'
            - '--prefix-caching-hash-algo=sha256'
            - '--served-model-name'
            - 'qwen3-coder-30b-a3b'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 64Gi
            limits:
              memory: 64Gi
        - name: inference-proxy-codegen-1
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8086
            - containerPort: 8186
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8086'
            - '--metrics-port=8186'
            - '--workload-port=8001'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=generate,tool_calling'
          volumeMounts:
            - name: inference-proxy-codegen-1-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-transcriptions-codegen
      volumes:
        - name: privatemode-ca-cert
          secret:
            secretName: privatemode-ca-secret
            items:
              - key: ca.crt
                path: ca.crt
        - emptyDir: {}
          name: mount-dir
        - emptyDir: {}
          name: continuum-run
        - name: vllm-audio-pylock
          configMap:
            name: vllm-audio-pylock
        - name: vllm-patch
          configMap:
            name: vllm-patch
        - name: inference-proxy-transcriptions-0-tls-cert
          secret:
            secretName: inference-proxy-transcriptions-0-tls-secret
        - name: inference-proxy-codegen-1-tls-cert
          secret:
            secretName: inference-proxy-codegen-1-tls-secret
        - name: modelweights-transcriptions-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: openai-whisper-large-v3-reproducible
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 10Gi
        - name: modelweights-codegen-1
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: stelterlab-qwen3-coder-30b-a3b-instruct-awq
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 18Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-voxtralmini
  namespace: continuum-c8cc26
  labels:
    app: workload-voxtralmini
    app.kubernetes.io/instance: 'c8cc26'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: workload-voxtralmini
  template:
    metadata:
      labels:
        app: workload-voxtralmini
        app.kubernetes.io/instance: 'c8cc26'
        privatemode.edgeless.systems/needs-gpu: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/scheme: "http"
    spec:
      serviceAccountName: workload-voxtralmini-sa
      runtimeClassName: contrast-cc
      initContainers:
        - name: attestation-agent
          volumeMounts:
            - name: privatemode-ca-cert
              mountPath: /etc/ssl/certs
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
            - mountPath: /contrast
              name: contrast-secrets
          image: ghcr.io/edgelesssys/privatemode/attestation-agent:v1.34.0@sha256:735407de699cfb96a51e78e59252e813e46390d8adf992b5ff062fa1c3e4516d
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          args:
            - --gpu-driver-versions=590.48.01
            - --gpu-vbios-versions=96.00.88.00.11,96.00.74.00.11,96.00.9F.00.04,96.00.74.00.1C
          securityContext:
            privileged: true
          resources:
            limits:
              "nvidia.com/GH100_H100_PCIE": 1
        - name: disk-mounter-voxtralmini-0
          image: ghcr.io/edgelesssys/privatemode/disk-mounter:v1.34.0@sha256:b262fa01f278716ddc1645d4be9d4955c2049322330a26b293d3bcef28c140cd
          command:
            - /bin/sh
            - -ec
            - |
              blockdev --setra 16384 /dev/modelweights-voxtralmini-0
              exec /bin/disk-mounter "$@"
            - _
          args:
            - --root-hash=8304092e55ff2c51ff3c897b0f575eee72a47f499aa3043dacdd19d0ec9ad989
            - --device-path=/dev/modelweights-voxtralmini-0
            - --mount-path=/mnt/modelweights-voxtralmini-0
          restartPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: Bidirectional
          volumeDevices:
            - name: modelweights-voxtralmini-0
              devicePath: /dev/modelweights-voxtralmini-0
      containers:
        - name: workload-voxtralmini-0
          image: docker.io/vllm/vllm-openai:v0.14.1@sha256:6bf34e50e2387dc46dc87a9d6a945fdd616a022bccfddd949052f54063ebcb8c
          ports:
            - containerPort: 8000
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
          env:
            - name: VLLM_MAX_AUDIO_CLIP_FILESIZE_MB
              value: "50"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          command:
            - /bin/sh
            - -ec
            - |
              cp /untrusted-pylock/pylock.toml /vllm-workspace/pylock.toml
              if [ $(sha256sum /vllm-workspace/pylock.toml | cut -d ' ' -f 1) != '4138144e81ab67708175d57b2fe717e0b8bdbe2bb6527b04eb65ef6aa94a0dcf' ]; then echo pylock.toml hash mismatch: $(sha256sum /vllm-workspace/pylock.toml | cut -d ' ' -f 1); exit 1; fi
              uv pip install -r /vllm-workspace/pylock.toml --system
              cp /untrusted-vllm-patch/transcriptions.patch /vllm-workspace/transcriptions.patch
              if [ $(sha256sum /vllm-workspace/transcriptions.patch | cut -d ' ' -f 1) != '9e006c923b049875f84acad59a74df9946eb721518ef0b3a909b0f52fd3119ff' ]; then echo transcriptions.patch hash mismatch: $(sha256sum /vllm-workspace/transcriptions.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 -p1 -d /usr/local/lib/python3.12/dist-packages < /vllm-workspace/transcriptions.patch
              cp /untrusted-vllm-patch/voxtral.patch /vllm-workspace/voxtral.patch
              if [ $(sha256sum /vllm-workspace/voxtral.patch | cut -d ' ' -f 1) != '39a315f02842090a671c9d1a6148db1b70079643efc4557772e78ab1f314761a' ]; then echo voxtral.patch hash mismatch: $(sha256sum /vllm-workspace/voxtral.patch | cut -d ' ' -f 1); exit 1; fi
              patch --batch --fuzz=0 -p1 -d /usr/local/lib/python3.12/dist-packages < /vllm-workspace/voxtral.patch
              exec python3 -m vllm.entrypoints.openai.api_server "$@"
            - _
          args:
            - '--host=0.0.0.0'
            - '--port=8000'
            - '--model=/mnt/modelweights-voxtralmini-0'
            - '--enable-prompt-tokens-details'
            - '--gpu-memory-utilization=0.1'
            - '--kv-cache-memory-bytes=20g'
            - '--max-num-seqs=20'
            - '--enable-auto-tool-choice'
            - '--tool-call-parser'
            - 'mistral'
            - '--tokenizer-mode'
            - 'mistral'
            - '--no-enable-prefix-caching'
            - '--served-model-name'
            - 'voxtral-mini-3b'
          volumeMounts:
            - name: mount-dir
              mountPath: /mnt
              mountPropagation: HostToContainer
            - name: vllm-audio-pylock
              mountPath: /untrusted-pylock
              readOnly: true
            - name: vllm-patch
              mountPath: /untrusted-vllm-patch
              readOnly: true
          resources:
            requests:
              memory: 70Gi
            limits:
              memory: 70Gi
              "nvidia.com/GH100_H100_PCIE": 1
        - name: inference-proxy-voxtralmini-0
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8085
            - containerPort: 8185
          args:
            - '--adapter-type=openai'
            - '--workload-address=127.0.0.1'
            - '--listen-port=8085'
            - '--metrics-port=8185'
            - '--workload-port=8000'
            - '--secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local'
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
            - '--workload-tasks=transcribe'
          volumeMounts:
            - name: inference-proxy-voxtralmini-0-tls-cert
              mountPath: /etc/tls
              readOnly: true
            - name: continuum-run
              mountPath: /var/run/continuum
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: workload-voxtralmini
      volumes:
        - name: privatemode-ca-cert
          secret:
            secretName: privatemode-ca-secret
            items:
              - key: ca.crt
                path: ca.crt
        - emptyDir: {}
          name: mount-dir
        - emptyDir: {}
          name: continuum-run
        - name: vllm-audio-pylock
          configMap:
            name: vllm-audio-pylock
        - name: vllm-patch
          configMap:
            name: vllm-patch
        - name: inference-proxy-voxtralmini-0-tls-cert
          secret:
            secretName: inference-proxy-voxtralmini-0-tls-secret
        - name: modelweights-voxtralmini-0
          ephemeral:
            volumeClaimTemplate:
              metadata: {}
              spec:
                storageClassName: mistralai-voxtral-mini-3b-2507
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unstructured-api
  namespace: continuum-c8cc26
  labels:
    app: unstructured-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: unstructured-api
  template:
    metadata:
      labels:
        app: unstructured-api
        app.kubernetes.io/instance: 'c8cc26'
    spec:
      runtimeClassName: contrast-cc
      containers:
        - name: unstructured-api
          image: 'quay.io/unstructured-io/unstructured-api:0.0.89@sha256:b2abf2f37acafcbefa62e74e508c3176c14929213ba5ae962d6bbe03234b15e5'
          command:
            - /bin/sh
            - -ec
            - |
              test -f logger_config.yaml
              cp /untrusted/logger_config.yaml .
              if [ $(sha256sum logger_config.yaml | cut -d ' ' -f 1) != '62b5acd1bff3662a27d27f4d263273447f596f2244bd35e92a450eb2247d79dd' ]; then
                echo "logger config hash mismatch: $(sha256sum logger_config.yaml | cut -d ' ' -f 1)"
                exit 1
              fi
              exec scripts/app-start.sh
          resources:
            requests:
              memory: "24Gi"
            limits:
              memory: "24Gi"
          volumeMounts:
            - name: unstructured-api-logging-config
              mountPath: /untrusted
              readOnly: true
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: http
              scheme: HTTP
            periodSeconds: 1
            failureThreshold: 2
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 5
        - name: inference-proxy
          image: ghcr.io/edgelesssys/privatemode/inference-proxy:v1.34.0@sha256:e0db3b01ee4697166a379c7c2f910c9d7017def2861a15e16be8f4e0f5292db3
          ports:
            - containerPort: 8085
          args:
            - --adapter-type=unstructured
            - --workload-address=127.0.0.1
            - --workload-port=8000
            - --secret-svc-address=secret-service-internal.continuum-c8cc26.svc.cluster.local
            - '--etcd-member-cert=/contrast/tls-config/certChain.pem'
            - '--etcd-member-key=/contrast/tls-config/key.pem'
            - '--etcd-ca=/contrast/tls-config/mesh-ca.pem'
          volumeMounts:
            - name: unstructured-api-tls-cert
              mountPath: /etc/tls
              readOnly: true
      volumes:
        - name: unstructured-api-tls-cert
          secret:
            secretName: unstructured-api-tls-secret
        - name: unstructured-api-logging-config
          configMap:
            name: unstructured-api-logging-config
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coordinator
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: coordinator
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - create
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: coordinator
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: coordinator
subjects:
  - kind: ServiceAccount
    name: coordinator
    namespace: "continuum-c8cc26"
---
apiVersion: v1
kind: Service
metadata:
  name: coordinator
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
spec:
  ports:
    - name: userapi
      port: 1313
      targetPort: 1313
    - name: httpapi
      port: 1314
      targetPort: 1314
    - name: meshapi
      port: 7777
      targetPort: 7777
    - name: transitapi
      port: 8200
      targetPort: 8200
    - name: default-userapi
      port: 443
      targetPort: 1313
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: coordinator
    app.kubernetes.io/instance: 'c8cc26'
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: coordinator-ready
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
spec:
  ports:
    - name: userapi
      port: 1313
      targetPort: 1313
    - name: httpapi
      port: 1314
      targetPort: 1314
    - name: meshapi
      port: 7777
      targetPort: 7777
    - name: transitapi
      port: 8200
      targetPort: 8200
    - name: default-userapi
      port: 443
      targetPort: 1313
  selector:
    app.kubernetes.io/name: coordinator
    app.kubernetes.io/instance: 'c8cc26'
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: coordinator
  namespace: 'continuum-c8cc26'
  labels:
    app.kubernetes.io/instance: 'c8cc26'
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Delete
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: coordinator
  serviceName: coordinator
  template:
    metadata:
      annotations:
        contrast.edgeless.systems/pod-role: coordinator
      labels:
        app.kubernetes.io/name: coordinator
        app.kubernetes.io/instance: 'c8cc26'
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    contrast.edgeless.systems/pod-role: coordinator
                topologyKey: kubernetes.io/hostname
              weight: 100
      containers:
        - image: ghcr.io/edgelesssys/contrast/coordinator:v1.16.0@sha256:59954b330835184a9ecd4604962b6404407c028f85e1b5dd6187ac56a1052e9d
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /probe/liveness
              port: 9102
            periodSeconds: 10
          name: coordinator
          ports:
            - containerPort: 1313
              name: userapi
            - containerPort: 1314
              name: httpapi
            - containerPort: 7777
              name: meshapi
            - containerPort: 8200
              name: transitapi
          readinessProbe:
            httpGet:
              path: /probe/readiness
              port: 9102
            periodSeconds: 5
          resources:
            limits:
              memory: 200Mi
            requests:
              memory: 200Mi
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /probe/startup
              port: 9102
            initialDelaySeconds: 1
            periodSeconds: 1
          env:
            - name: CONTRAST_LOG_FORMAT
              value: json
      runtimeClassName: contrast-cc
      serviceAccountName: coordinator
